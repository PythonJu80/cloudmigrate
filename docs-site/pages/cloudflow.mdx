# CloudFlow

CloudFlow is a visual workflow builder for multi-cloud automation.

## Node Categories

CloudFlow includes nodes across four cloud providers plus generic utilities.

### Triggers

| Node | Description |
|------|-------------|
| Manual Trigger | Start workflow with button click |
| Schedule (Cron) | Run on schedule with timezone support |
| Webhook | Trigger via HTTP POST/GET/PUT |

### AI Nodes

| Node | Description |
|------|-------------|
| Workflow Agent | AI that builds CloudFlow workflows |
| LLM Chat | Send prompts to GPT models |
| LLM Completion | Generate text completions |

### AWS Nodes

| Category | Nodes |
|----------|-------|
| Compute | EC2 Create/Start/Stop/Terminate, Lambda Invoke |
| Storage | S3 Upload/Download/List/Delete, EFS operations |
| Database | RDS Query, DynamoDB Get/Put/Query/Scan |
| Integration | SQS Send/Receive, SNS Publish, EventBridge Put |
| Networking | Route53, API Gateway |
| Monitoring | CloudWatch Metrics/Logs/Alarms |
| AI/ML | Bedrock Invoke, SageMaker, Rekognition, Comprehend |

### GCP Nodes

| Category | Nodes |
|----------|-------|
| Compute | Compute Engine, Cloud Functions, Cloud Run |
| Storage | Cloud Storage Upload/Download/List |
| Database | Cloud SQL Query, Firestore, BigQuery |
| Integration | Pub/Sub Publish/Subscribe, Cloud Tasks |
| AI/ML | Vertex AI, Gemini, Vision API, Speech-to-Text |

### Azure Nodes

| Category | Nodes |
|----------|-------|
| Compute | Virtual Machines, Functions, Container Apps |
| Storage | Blob Upload/Download/List |
| Database | SQL Database, Cosmos DB |
| Integration | Event Grid, Service Bus |
| AI/ML | Azure OpenAI, Computer Vision, Speech |

### Oracle Cloud Nodes

| Category | Nodes |
|----------|-------|
| Compute | Virtual Machines, OKE, OCI Functions |
| Storage | Object Storage operations |
| Database | ATP Query, ADW, NoSQL |
| Analytics | Streaming, Data Integration |
| AI/ML | Data Science, AI Vision, Speech |

### Logic Nodes

| Node | Description |
|------|-------------|
| Condition | If/else branching |
| Switch | Multi-way branching |
| Loop | Iterate over arrays |
| Delay | Wait for specified duration |
| Parallel | Run branches concurrently |
| Error Handler | Try/catch with retry |

### Local File Nodes

| Node | Description |
|------|-------------|
| Read File | Read local file contents |
| Write File | Write data to local file |
| List Directory | List files in folder |
| Parse CSV | Parse CSV to JSON |
| Parse JSON | Parse JSON file |
| Transform Data | Map/filter/reduce operations |

### Output Nodes

| Node | Description |
|------|-------------|
| Return Response | Return data to webhook caller |
| Log Output | Store in execution history |
| Send to Webhook | POST to external URL |

## Creating Workflows

1. Drag a trigger node onto the canvas
2. Add action nodes from the sidebar
3. Connect nodes by dragging between ports
4. Click each node to configure its settings
5. Save and run

## Scheduling

### Cron Expressions

Schedule workflows using cron syntax:

```
# Every day at midnight
0 0 * * *

# Every hour
0 * * * *

# Every Monday at 9am
0 9 * * 1

# Every 15 minutes
*/15 * * * *
```

### Event Triggers

Trigger workflows based on events:

- **S3 Event** - New file uploaded to bucket
- **Webhook** - HTTP request received
- **Schedule** - Cron-based timing
- **Manual** - User-initiated execution
- **Workflow** - Another workflow completed

## Execution

### Running Workflows

Execute workflows:

- **Manual** - Click Run button
- **Scheduled** - Automatic based on cron
- **Triggered** - Automatic based on events
- **API** - POST to `/api/workflows/{id}/run`

### Execution States

| State | Description |
|-------|-------------|
| Pending | Queued for execution |
| Running | Currently executing |
| Completed | Finished successfully |
| Failed | Encountered error |
| Cancelled | Manually stopped |

### Viewing Logs

Each execution provides:

- Start and end timestamps
- Duration
- Node-by-node execution trace
- Input/output data samples
- Error messages and stack traces

## Error Handling

### Retry Configuration

Configure retry behavior per node:

```json
{
  "retries": 3,
  "backoff": "exponential",
  "initialDelay": 1000,
  "maxDelay": 30000
}
```

### Error Nodes

Use Error Handler nodes to:

- Catch specific error types
- Execute fallback logic
- Send alert notifications
- Log to external systems

### Dead Letter Queue

Failed executions can be:

- Automatically retried
- Sent to dead letter queue
- Trigger alert workflows

## Variables and Secrets

### Workflow Variables

Define variables for reuse:

```json
{
  "variables": {
    "bucket": "my-data-bucket",
    "prefix": "daily-exports/",
    "recipients": ["team@company.com"]
  }
}
```

Reference in nodes: `{{variables.bucket}}`

### Secrets

Sensitive values are stored encrypted:

1. Go to **Settings** â†’ **Secrets**
2. Add a new secret
3. Reference in workflows: `{{secrets.API_KEY}}`

Secrets are never logged or exposed in the UI.

## Best Practices

1. **Start simple** - Build incrementally, test each node
2. **Use error handlers** - Always plan for failure
3. **Monitor executions** - Set up alerts for failures
4. **Version workflows** - Use descriptive names and comments
5. **Test with samples** - Validate with small datasets first
6. **Secure secrets** - Never hardcode credentials
