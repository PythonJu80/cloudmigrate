# MCP Crawl4AI-RAG Server Enhancements

## Overview
Simple, powerful enhancements that actually matter. No bullshit job management or progress tracking nonsense.

## üöÄ What's Actually New

### 1. Rate Limiting ‚ö°
**Purpose**: Prevent abuse and ensure fair resource usage

**How it works**:
- Token bucket algorithm for smooth rate limiting
- Configurable limits via environment variables
- Graceful error messages with wait times

**Configuration** (`.env`):
```env
USE_RATE_LIMITING=true
RATE_LIMIT_REQUESTS=10    # Max requests
RATE_LIMIT_PERIOD=60      # Per 60 seconds
```

**Example Error Response**:
```json
{
  "success": false,
  "error": "Rate limit exceeded",
  "wait_time_seconds": 5.2,
  "message": "Please wait 5.2s before making another request"
}
```

---

### 2. The Killer Tool: `crawl_urls` üî•
**Purpose**: ONE tool that does EVERYTHING

**What it does**:
- Takes 1 URL or 100 URLs - doesn't matter
- Crawls them ALL in parallel (fast as hell)
- Extracts content
- Chunks intelligently
- Generates embeddings
- Stores in Supabase
- Returns complete results

**Single URL**:
```json
{
  "urls": "https://docs.python.org/3/tutorial/"
}
```

**Multiple URLs**:
```json
{
  "urls": [
    "https://site1.com",
    "https://site2.com",
    "https://site3.com"
  ],
  "max_concurrent": 10,
  "chunk_size": 5000
}
```

**Response**:
```json
{
  "success": true,
  "summary": {
    "urls_requested": 3,
    "urls_succeeded": 3,
    "urls_failed": 0,
    "total_chunks_created": 147,
    "total_chunks_stored": 147,
    "total_content_length": 735000
  },
  "results": [
    {
      "url": "https://site1.com",
      "source": "site1.com",
      "chunks_created": 52,
      "chunks_stored": 52,
      "content_length": 260000,
      "title": "Site 1 Title"
    },
    ...
  ],
  "message": "‚úÖ Processed 3/3 URLs successfully. Created 147 searchable chunks."
}
```

**Why it's killer**:
- ‚úÖ Handles both single and multiple URLs
- ‚úÖ Parallel processing with concurrency control
- ‚úÖ Complete results - no polling needed
- ‚úÖ Automatic chunking and embedding
- ‚úÖ All-in-one: crawl ‚Üí chunk ‚Üí embed ‚Üí store
- ‚úÖ Perfect for documentation, articles, research papers

---

## üîß Implementation Details

### Architecture Decisions

1. **Thread-Safe Components**: All new components (RateLimiter, CrawlProgressTracker, WebhookNotifier) use threading locks for thread safety

2. **Non-Breaking Changes**: All existing tools work exactly as before. New features are additive only.

3. **Context Integration**: New components are added to the `Crawl4AIContext` dataclass and initialized during lifespan

4. **Background Processing**: Batch crawls run in background tasks using asyncio, allowing immediate response to user

5. **Graceful Degradation**: If rate limiting is disabled, all tools work normally without any restrictions

### Resource Management

- **Memory**: Progress tracker stores job data in memory with automatic cleanup
- **Network**: Webhook notifications use async HTTP with 5-second timeout
- **Concurrency**: Batch operations respect `max_concurrent` parameter to avoid overwhelming the system

---

## üìñ Usage Examples

### Basic Single Crawl (Existing - No Changes)
```python
await crawl_single_page(ctx, url="https://example.com")
```

### Batch Crawl with Progress Tracking
```python
# Start batch crawl
result = await batch_crawl_urls(
    ctx,
    urls=["https://site1.com", "https://site2.com"],
    max_concurrent=5,
    webhook_url="https://myapp.com/webhook"
)
job_id = json.loads(result)["job_id"]

# Check progress
progress = await get_crawl_progress(ctx, job_id=job_id)

# List all recent jobs
jobs = await list_batch_jobs(ctx, limit=10)
```

### Webhook Integration Example (Node.js/Express)
```javascript
app.post('/webhook', (req, res) => {
  const { job_id, event, data } = req.body;
  
  if (event === 'batch_crawl_completed') {
    console.log(`Job ${job_id} completed!`);
    console.log(`Success: ${data.completed}, Failed: ${data.failed}`);
    
    // Trigger next steps in your workflow
    processCompletedCrawl(data);
  }
  
  res.sendStatus(200);
});
```

---

## üõ°Ô∏è Safety & Best Practices

### Rate Limiting Best Practices
- Set reasonable limits based on your infrastructure
- Monitor wait times to tune limits
- Consider different limits for different environments

### Batch Operations Best Practices
- Don't batch too many URLs at once (recommended: < 100)
- Use appropriate `max_concurrent` (recommended: 5-10)
- Implement webhook endpoints with retry logic
- Store job IDs for later reference

### Progress Tracking Best Practices
- Poll progress at reasonable intervals (e.g., every 5-10 seconds)
- Handle `job_id not found` errors gracefully
- Clean up completed jobs if storing references

---

## üß™ Testing

### Test Rate Limiting
```bash
# Make rapid requests to trigger rate limit
for i in {1..20}; do
  curl -X POST http://localhost:8051/crawl_single_page \
    -d '{"url": "https://example.com"}'
done
```

### Test Batch Crawl
```bash
curl -X POST http://localhost:8051/batch_crawl_urls \
  -H "Content-Type: application/json" \
  -d '{
    "urls": ["https://site1.com", "https://site2.com"],
    "max_concurrent": 5
  }'
```

---

## üîÑ Migration Guide

**Good news**: No migration needed! All existing code continues to work exactly as before.

**To adopt new features**:

1. **Enable rate limiting** (optional):
   ```env
   USE_RATE_LIMITING=true
   RATE_LIMIT_REQUESTS=10
   RATE_LIMIT_PERIOD=60
   ```

2. **Use batch operations** (optional):
   - Replace multiple `crawl_single_page` calls with one `batch_crawl_urls`
   - Use `get_crawl_progress` to monitor status

3. **Add webhook support** (optional):
   - Create webhook endpoint in your application
   - Pass `webhook_url` parameter to `batch_crawl_urls`

---

## üìù Summary

These enhancements make the MCP Crawl4AI-RAG server more robust and production-ready without breaking any existing functionality. They provide:

‚úÖ Protection against abuse via rate limiting  
‚úÖ Efficient batch processing of multiple URLs  
‚úÖ Real-time progress tracking  
‚úÖ Event-driven notifications via webhooks  

All features are **optional** and **configurable**, allowing you to adopt them at your own pace while maintaining full backward compatibility.
