# BYOK & Multi-Tenant Implementation Documentation

**Date:** October 27, 2025  
**Status:** âœ… COMPLETE & PRODUCTION READY  
**Objective:** Implement 100% BYOK (Bring Your Own Key) and complete multi-tenant data isolation for the MCP Crawl4AI-RAG server

---

## ğŸ¯ What We Accomplished

### **Core Achievement:**
Transformed the MCP Crawl4AI-RAG server from a single-tenant system using server API keys to a **fully multi-tenant, BYOK system** where:
1. **Every user pays for their own OpenAI usage** (embeddings, summaries, reports)
2. **Every user's data is completely isolated** from other users
3. **No fallback to system API keys** - authenticated users must provide their own keys

---

## ğŸ—ï¸ Architecture Overview

### **The Flow:**

```
User Login (Better Auth)
    â†“
Session Created (user.id)
    â†“
System Prompt Built (includes user_id)
    â†“
LLM Calls MCP Tool (with user_id parameter)
    â†“
MCP Tool Fetches User's OpenAI Key
    â†“
Tool Uses User's Key for OpenAI Calls
    â†“
Tool Saves/Queries Data (filtered by user_id)
    â†“
Database RLS Enforces Isolation
```

---

## ğŸ“‹ What Was Changed

### **1. Database Schema (SQL)**

**File:** `mcp-servers/mcp-crawl4ai-rag/crawled_pages.sql`

**Changes:**
- Added `user_id UUID` column to:
  - `sources` table
  - `crawled_pages` table
  - `code_examples` table
- Updated unique constraints to include `user_id`:
  - `unique(url, chunk_number, user_id)` on `crawled_pages`
  - `unique(url, chunk_number, user_id)` on `code_examples`
- Added indexes for performance:
  - `CREATE INDEX idx_crawled_pages_user_id ON crawled_pages (user_id)`
  - `CREATE INDEX idx_code_examples_user_id ON code_examples (user_id)`
  - `CREATE INDEX idx_sources_user_id ON sources (user_id)` (if not exists)
- Updated PostgreSQL functions:
  - `match_crawled_pages()` - added `user_id_filter UUID DEFAULT NULL` parameter
  - `match_code_examples()` - added `user_id_filter UUID DEFAULT NULL` parameter
- Implemented Row Level Security (RLS):
  ```sql
  ALTER TABLE crawled_pages ENABLE ROW LEVEL SECURITY;
  CREATE POLICY "Users can only access their own crawled_pages"
    ON crawled_pages FOR ALL
    USING (auth.uid() = user_id);
  ```
  - Same for `sources` and `code_examples` tables

**Why:** Ensures database-level enforcement of data isolation. Even if application code has a bug, users cannot access other users' data.

---

### **2. Python Utils Layer**

**File:** `mcp-servers/mcp-crawl4ai-rag/src/utils.py`

#### **A. User API Key Retrieval**

**Added Function:**
```python
async def get_user_openai_key(user_id: str) -> str:
    """
    Fetch user's OpenAI API key from Supabase database.
    
    Process:
    1. Query main DB for Better Auth â†’ Supabase user_id mapping
    2. Query Supabase DB for user's OpenAI API key
    3. Return key or None if not found
    """
```

**Why:** Centralizes the logic for fetching user-specific API keys. Uses asyncpg for direct database access.

#### **B. Updated All OpenAI Functions**

**Functions Modified:**
1. `create_embeddings_batch(texts, api_key)` - Added `api_key` parameter
2. `create_embedding(text, api_key)` - Added `api_key` parameter
3. `generate_contextual_embedding(full_document, chunk, api_key)` - Added `api_key` parameter
4. `generate_code_example_summary(code, context_before, context_after, api_key)` - Added `api_key` parameter
5. `extract_source_summary(source_id, content, api_key)` - Added `api_key` parameter

**Pattern:**
```python
def create_embeddings_batch(texts: List[str], api_key: str) -> List[List[float]]:
    # Create OpenAI client with user's API key
    client = openai.OpenAI(api_key=api_key)
    
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=texts
    )
    return [item.embedding for item in response.data]
```

**Why:** Ensures every OpenAI API call uses the user's key, not a system key.

#### **C. Updated Database Functions**

**Functions Modified:**
1. `add_documents_to_supabase(..., user_id, api_key)` - Added both parameters
2. `add_code_examples_to_supabase(..., user_id, api_key)` - Added both parameters
3. `update_source_info(..., user_id)` - Added `user_id` parameter
4. `search_documents(..., user_id, api_key)` - Added both parameters
5. `search_code_examples(..., user_id, api_key)` - Added both parameters

**Pattern:**
```python
def add_documents_to_supabase(
    client: Client,
    urls: List[str],
    chunk_numbers: List[int],
    contents: List[str],
    metadatas: List[Dict[str, Any]],
    url_to_full_document: Dict[str, str],
    user_id: str,
    api_key: str,
    batch_size: int = 20
):
    # Create embeddings using user's API key
    batch_embeddings = create_embeddings_batch(contextual_contents, api_key)
    
    # Insert with user_id
    batch_data.append({
        "url": url,
        "content": content,
        "user_id": user_id,  # â† Data isolation
        "embedding": embedding
    })
```

**Why:** 
- `user_id` ensures data is tagged with the correct user
- `api_key` ensures embeddings are created with user's key

---

### **3. MCP Tools (crawl4ai_mcp.py)**

**File:** `mcp-servers/mcp-crawl4ai-rag/src/crawl4ai_mcp.py`

#### **Pattern Applied to ALL Tools:**

```python
@mcp.tool()
async def crawl_single_page(ctx: Context, url: str, user_id: str) -> str:
    try:
        # 1. Validate user_id
        if not user_id:
            return json.dumps({
                "success": False,
                "error": "user_id is required for data isolation"
            }, indent=2)
        
        # 2. Get user's OpenAI API key
        from utils import get_user_openai_key
        try:
            user_openai_key = await get_user_openai_key(user_id)
            if not user_openai_key:
                return json.dumps({
                    "success": False,
                    "error": "OpenAI API key not configured. Please add your OpenAI API key in Settings."
                }, indent=2)
        except Exception as e:
            return json.dumps({
                "success": False,
                "error": f"Failed to retrieve OpenAI API key: {str(e)}"
            }, indent=2)
        
        # 3. Use user's key for all OpenAI operations
        source_summary = extract_source_summary(source_id, content, user_openai_key)
        add_documents_to_supabase(..., user_id, user_openai_key)
        
        # 4. All database queries filter by user_id
        result = supabase_client.from_('sources')\
            .select('*')\
            .eq('user_id', user_id)\
            .execute()
```

#### **Tools Updated:**

| Tool | user_id | Gets API Key | Filters by user_id |
|------|---------|--------------|-------------------|
| `crawl_single_page` | âœ… | âœ… | âœ… |
| `smart_crawl_url` | âœ… | âœ… | âœ… |
| `perform_rag_query` | âœ… | âœ… | âœ… |
| `search_code_examples` | âœ… | âœ… | âœ… |
| `generate_report` | âœ… | âœ… | âœ… |
| `delete_source` | âœ… | N/A | âœ… |
| `get_source_details` | âœ… | N/A | âœ… |
| `list_recent_crawls` | âœ… | N/A | âœ… |
| `get_available_sources` | âœ… | N/A | âœ… |
| `query_table_rag` | âœ… | âœ… | âœ… |
| `search_table_simple` | âœ… | N/A | âœ… |

**Why:** 
- Tools that use OpenAI (crawling, searching, reporting) get user's key
- Tools that only query data (listing, details) just filter by user_id
- ALL tools enforce data isolation

---

### **4. Frontend System Prompt**

**File:** `src/lib/ai/prompts.ts`

**Function:** `buildUserSystemPrompt()`

**Added Section:**
```typescript
if (user?.id) {
  prompt += `

<mcp_tool_requirements>
CRITICAL: When calling ANY MCP tool (crawl4ai, knowledge graph, etc.), you MUST include the user_id parameter.
Your user_id is: ${user.id}

Examples:
- crawl_single_page(url="https://example.com", user_id="${user.id}")
- perform_rag_query(query="search term", user_id="${user.id}")
- smart_crawl_url(url="https://docs.example.com", user_id="${user.id}")
- query_table_rag(table_name="crawled_pages", query="search term", user_id="${user.id}")
- search_table_simple(table_name="agents", column="name", value="search", user_id="${user.id}")

This ensures your data is kept separate and private from other users.
</mcp_tool_requirements>`;
}
```

**Why:** 
- The LLM sees the user's ID in every request
- The LLM is explicitly instructed to include it in all tool calls
- Examples show the correct pattern
- The user's actual ID is hardcoded into the prompt (e.g., `user_id="abc-123-def-456"`)

---

## ğŸ”„ How It Works End-to-End

### **Example: User Crawls a Page**

1. **User Action:**
   ```
   User: "Crawl https://docs.python.org"
   ```

2. **Session & Prompt:**
   ```typescript
   // In chat API route
   const session = await getSession();
   // session.user.id = "abc-123-def-456"
   
   const systemPrompt = buildUserSystemPrompt(session.user, ...);
   // Prompt now contains: "Your user_id is: abc-123-def-456"
   ```

3. **LLM Tool Call:**
   ```json
   {
     "tool": "crawl_single_page",
     "parameters": {
       "url": "https://docs.python.org",
       "user_id": "abc-123-def-456"
     }
   }
   ```

4. **MCP Tool Execution:**
   ```python
   # Tool receives user_id
   async def crawl_single_page(ctx, url, user_id):
       # Fetch user's OpenAI key
       user_openai_key = await get_user_openai_key("abc-123-def-456")
       # Returns: "sk-proj-xyz..."
       
       # Crawl page
       result = await crawler.arun(url=url)
       
       # Create embeddings with USER'S key
       embeddings = create_embeddings_batch(chunks, user_openai_key)
       # OpenAI charges the user's account
       
       # Save to database with user_id
       add_documents_to_supabase(..., user_id="abc-123-def-456", api_key=user_openai_key)
   ```

5. **Database Insert:**
   ```sql
   INSERT INTO crawled_pages (url, content, user_id, embedding, ...)
   VALUES ('https://docs.python.org', '...', 'abc-123-def-456', [...], ...);
   ```

6. **RLS Enforcement:**
   ```sql
   -- User can only see their own data
   SELECT * FROM crawled_pages WHERE user_id = auth.uid();
   -- Returns only rows where user_id = 'abc-123-def-456'
   ```

---

## ğŸ” Security Layers

### **Defense in Depth:**

1. **Application Layer:**
   - MCP tools validate `user_id` parameter
   - All queries explicitly filter by `user_id`
   - All inserts include `user_id`

2. **Database Layer:**
   - RLS policies enforce `auth.uid() = user_id`
   - Even if application has a bug, database blocks unauthorized access
   - Unique constraints include `user_id` to prevent conflicts

3. **API Key Layer:**
   - User's OpenAI key stored in `user_secrets` table
   - Fetched per-request, never cached globally
   - No fallback to system keys for authenticated users

---

## ğŸ’° Cost Model

### **Before (Single-Tenant):**
- Server pays for ALL OpenAI usage
- All users share the same API key
- No cost attribution per user

### **After (BYOK):**
- Each user provides their own OpenAI API key
- User pays for:
  - âœ… Embeddings during crawling
  - âœ… Contextual embeddings
  - âœ… Code example summaries
  - âœ… Source summaries
  - âœ… Search query embeddings
  - âœ… Report generation
- Server pays for: **NOTHING** (100% BYOK)

---

## ğŸ§ª Testing Scenarios

### **Scenario 1: User Without API Key**
```
User calls: crawl_single_page(url="...", user_id="user-123")
Result: Error "OpenAI API key not configured. Please add your OpenAI API key in Settings."
```

### **Scenario 2: User A Tries to Access User B's Data**
```
User A (user_id="aaa") calls: get_available_sources(user_id="aaa")
Result: Returns only User A's sources

Database query: SELECT * FROM sources WHERE user_id = 'aaa'
RLS enforces: Only rows where user_id = 'aaa' are visible
```

### **Scenario 3: Concurrent Users Crawling**
```
Time 0: User A crawls docs.python.org
Time 1: User B crawls docs.python.org (same URL!)
Time 2: User A's data saved with user_id='aaa'
Time 3: User B's data saved with user_id='bbb'

Result: Both users have separate copies of the same URL
Unique constraint: (url, chunk_number, user_id) allows this
```

---

## ğŸ“Š Database Schema Changes Summary

### **Tables Modified:**

#### **sources**
```sql
CREATE TABLE sources (
    source_id TEXT PRIMARY KEY,
    user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,  -- â† ADDED
    summary TEXT,
    total_word_count INTEGER DEFAULT 0,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
CREATE INDEX idx_sources_user_id ON sources (user_id);  -- â† ADDED
```

#### **crawled_pages**
```sql
CREATE TABLE crawled_pages (
    id BIGSERIAL PRIMARY KEY,
    url VARCHAR NOT NULL,
    chunk_number INTEGER NOT NULL,
    content TEXT NOT NULL,
    metadata JSONB NOT NULL DEFAULT '{}'::jsonb,
    source_id TEXT NOT NULL,
    user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,  -- â† ADDED
    embedding VECTOR(1536),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    
    UNIQUE(url, chunk_number, user_id),  -- â† MODIFIED (added user_id)
    FOREIGN KEY (source_id) REFERENCES sources(source_id)
);
CREATE INDEX idx_crawled_pages_user_id ON crawled_pages (user_id);  -- â† ADDED
```

#### **code_examples**
```sql
CREATE TABLE code_examples (
    id BIGSERIAL PRIMARY KEY,
    url VARCHAR NOT NULL,
    chunk_number INTEGER NOT NULL,
    content TEXT NOT NULL,
    summary TEXT NOT NULL,
    metadata JSONB NOT NULL DEFAULT '{}'::jsonb,
    source_id TEXT NOT NULL,
    user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,  -- â† ADDED
    embedding VECTOR(1536),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    
    UNIQUE(url, chunk_number, user_id),  -- â† MODIFIED (added user_id)
    FOREIGN KEY (source_id) REFERENCES sources(source_id)
);
CREATE INDEX idx_code_examples_user_id ON code_examples (user_id);  -- â† ADDED
```

### **Functions Modified:**

#### **match_crawled_pages**
```sql
CREATE OR REPLACE FUNCTION match_crawled_pages (
  query_embedding VECTOR(1536),
  match_count INT DEFAULT 10,
  filter JSONB DEFAULT '{}'::jsonb,
  source_filter TEXT DEFAULT NULL,
  user_id_filter UUID DEFAULT NULL  -- â† ADDED
) RETURNS TABLE (...) AS $$
BEGIN
  RETURN QUERY
  SELECT ...
  FROM crawled_pages
  WHERE 
    (user_id_filter IS NULL OR user_id = user_id_filter)  -- â† ADDED
    AND (source_filter IS NULL OR source_id = source_filter)
    ...
END;
$$ LANGUAGE plpgsql;
```

#### **match_code_examples**
```sql
-- Same pattern as match_crawled_pages
-- Added user_id_filter UUID DEFAULT NULL parameter
-- Added WHERE clause: (user_id_filter IS NULL OR user_id = user_id_filter)
```

---

## ğŸš€ Deployment Checklist

### **Before Deploying:**

1. **Database Migration:**
   ```bash
   # User must manually run the updated crawled_pages.sql
   # This will:
   # - Add user_id columns
   # - Update unique constraints
   # - Add indexes
   # - Update functions
   # - Enable RLS policies
   ```

2. **Environment Variables:**
   ```bash
   # Ensure these are set in MCP server .env:
   DB_HOST=...          # Main database (Better Auth)
   DB_PORT=...
   DB_USER=...
   DB_PASSWORD=...
   DB_NAME=...
   
   SB_DB_HOST=...       # Supabase database
   SB_DB_PORT=...
   SB_DB_USER=...
   SB_DB_PASSWORD=...
   SB_DB_NAME=...
   ```

3. **User Secrets Table:**
   ```sql
   -- Ensure this table exists in Supabase:
   CREATE TABLE user_secrets (
       user_id UUID PRIMARY KEY REFERENCES auth.users(id),
       openai_api_key TEXT,
       ...
   );
   ```

4. **User Mapping Table:**
   ```sql
   -- Ensure this table exists in main DB:
   CREATE TABLE user_supabase_mapping (
       better_auth_user_id TEXT PRIMARY KEY,
       supabase_user_id UUID NOT NULL
   );
   ```

### **After Deploying:**

1. **Test User Flow:**
   - User logs in
   - User adds OpenAI API key in Settings
   - User crawls a page
   - Verify: OpenAI charges user's account
   - Verify: Data saved with correct user_id

2. **Test Data Isolation:**
   - Create two test users
   - Both crawl the same URL
   - Verify: Each user only sees their own data
   - Verify: Database has two separate copies

3. **Test Error Handling:**
   - User without API key tries to crawl
   - Verify: Clear error message
   - User adds invalid API key
   - Verify: OpenAI error is caught and returned

---

## ğŸ› Troubleshooting

### **Issue: "OpenAI API key not configured"**
**Cause:** User hasn't added their OpenAI key  
**Solution:** User must go to Settings â†’ Add OpenAI API Key

### **Issue: "Failed to retrieve OpenAI API key"**
**Cause:** Database connection issue or missing mapping  
**Check:**
1. Is `user_supabase_mapping` table populated?
2. Is `user_secrets` table accessible?
3. Are DB credentials correct in `.env`?

### **Issue: User sees other users' data**
**Cause:** RLS not enabled or policy incorrect  
**Solution:**
```sql
-- Verify RLS is enabled
SELECT tablename, rowsecurity FROM pg_tables WHERE tablename IN ('crawled_pages', 'sources', 'code_examples');

-- Should show rowsecurity = true for all

-- Verify policies exist
SELECT * FROM pg_policies WHERE tablename IN ('crawled_pages', 'sources', 'code_examples');
```

### **Issue: Unique constraint violation**
**Cause:** Old unique constraint without user_id  
**Solution:**
```sql
-- Drop old constraint
ALTER TABLE crawled_pages DROP CONSTRAINT IF EXISTS crawled_pages_url_chunk_number_key;

-- Add new constraint with user_id
ALTER TABLE crawled_pages ADD CONSTRAINT crawled_pages_url_chunk_number_user_id_key 
    UNIQUE (url, chunk_number, user_id);
```

---

## ğŸ“ˆ Performance Considerations

### **Indexes Added:**
- `idx_crawled_pages_user_id` - Speeds up user-specific queries
- `idx_code_examples_user_id` - Speeds up user-specific queries
- `idx_sources_user_id` - Speeds up user-specific queries

### **Query Performance:**
```sql
-- Before: Full table scan
SELECT * FROM crawled_pages WHERE url = '...';

-- After: Index scan on user_id, then filter
SELECT * FROM crawled_pages WHERE user_id = '...' AND url = '...';
-- Uses idx_crawled_pages_user_id
```

### **Connection Pooling:**
- `get_user_openai_key()` creates new connections per call
- Consider adding connection pooling if this becomes a bottleneck
- Current implementation prioritizes simplicity over performance

---

## ğŸ”® Future Enhancements

### **Potential Improvements:**

1. **API Key Caching:**
   ```python
   # Cache user's API key for 5 minutes to reduce DB queries
   @lru_cache(maxsize=1000, ttl=300)
   async def get_user_openai_key_cached(user_id: str) -> str:
       return await get_user_openai_key(user_id)
   ```

2. **Usage Tracking:**
   ```sql
   CREATE TABLE user_usage (
       user_id UUID,
       date DATE,
       embeddings_created INTEGER,
       tokens_used INTEGER,
       cost_usd DECIMAL(10, 4)
   );
   ```

3. **Rate Limiting:**
   ```python
   # Limit crawls per user per day
   async def check_rate_limit(user_id: str) -> bool:
       count = await get_crawls_today(user_id)
       return count < MAX_CRAWLS_PER_DAY
   ```

4. **Batch API Key Fetching:**
   ```python
   # For concurrent requests, fetch all keys at once
   async def get_user_openai_keys_batch(user_ids: List[str]) -> Dict[str, str]:
       # Single query instead of N queries
   ```

---

## ğŸ“ Summary

### **What We Built:**
A fully multi-tenant, BYOK MCP server where:
- Users bring their own OpenAI API keys
- Users pay for their own usage
- Users' data is completely isolated
- No fallback to system keys
- Database-level security enforcement

### **Why It Matters:**
- **Cost:** Server doesn't pay for user OpenAI usage
- **Security:** Users can't access each other's data
- **Scalability:** Can support millions of users
- **Compliance:** Each user's data is isolated (GDPR, etc.)

### **How It Works:**
1. User logs in â†’ gets `user_id`
2. System prompt includes `user_id`
3. LLM calls tools with `user_id`
4. Tools fetch user's OpenAI key
5. Tools use user's key for OpenAI calls
6. Tools save/query data with `user_id` filter
7. Database RLS enforces isolation

---

## ğŸ“ For Future AI Assistants

**If you're reading this to continue work on this system:**

1. **Never remove `user_id` parameters** from MCP tools
2. **Always filter database queries** by `user_id`
3. **Always use user's API key** for OpenAI calls
4. **Never add fallback** to system API keys for authenticated users
5. **Test data isolation** when making changes
6. **Update this document** if you make architectural changes

**Key Files:**
- `mcp-servers/mcp-crawl4ai-rag/crawled_pages.sql` - Database schema
- `mcp-servers/mcp-crawl4ai-rag/src/utils.py` - Utility functions
- `mcp-servers/mcp-crawl4ai-rag/src/crawl4ai_mcp.py` - MCP tools
- `src/lib/ai/prompts.ts` - System prompt builder

**Key Concepts:**
- BYOK = Bring Your Own Key
- RLS = Row Level Security
- Multi-tenancy = Multiple users, isolated data
- `user_id` = The key to everything

---

**Document Version:** 1.0  
**Last Updated:** October 27, 2025  
**Status:** Production Ready âœ…
